<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-GVM3KG39NS"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-GVM3KG39NS');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Understanding the difference between forward and reverse KL divergence in LLM training, and why the direction matters for RLHF and distillation.">
    <title>Forward vs Reverse KL in LLM Training | Hassan Hamad</title>
    <link rel="icon" type="image/x-icon" href="../../assets/img/favicon.ico">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="../../css/styles.css?v=2">
    <link rel="stylesheet" href="../../css/blog.css?v=2">
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}]});"></script>
    <!-- Prism.js for syntax highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <a href="../../index.html" class="nav-logo">Hassan Hamad</a>
            <button class="nav-toggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            <ul class="nav-menu">
                <li><a href="../../index.html#about">About</a></li>
                <li><a href="../../index.html#news">News</a></li>
                <li><a href="../../index.html#research">Research</a></li>
                <li><a href="../../index.html#blog">Blog</a></li>
                <li><a href="../../assets/HassanHamad_CV.pdf" target="_blank">CV</a></li>
            </ul>
        </div>
    </nav>

    <article class="blog-post">
        <!-- Table of Contents Sidebar -->
        <nav class="toc-sidebar" id="toc">
            <div class="toc-title">Contents</div>
            <ul class="toc-list">
                <li><a href="#what-is-kl" class="toc-link">What is KL Divergence?</a></li>
                <li><a href="#experiment" class="toc-link">The Experiment</a>
                    <ul>
                        <li><a href="#setup" class="toc-link">Setting Up Distributions</a></li>
                        <li><a href="#computing-kl" class="toc-link">Computing KL</a></li>
                        <li><a href="#forward-kl" class="toc-link">Forward KL</a></li>
                        <li><a href="#reverse-kl" class="toc-link">Reverse KL</a></li>
                    </ul>
                </li>
                <li><a href="#takeaway" class="toc-link">The Takeaway</a></li>
                <li><a href="#quiz" class="toc-link">Quiz Time!</a></li>
                <li><a href="#more-examples" class="toc-link">More Examples</a></li>
                <li><a href="#estimating-kl" class="toc-link">Estimating KL in Practice</a></li>
                <li><a href="#citation" class="toc-link">Citation</a></li>
            </ul>
        </nav>

        <div class="blog-post-container">
            <a href="../../index.html#blog" class="back-link">
                <i class="fas fa-arrow-left"></i> Back to Homepage
            </a>

            <header class="blog-post-header">
                <h1>Forward vs Reverse KL Divergence: Why the Direction Matters</h1>
                <p class="blog-post-subtitle">Understanding mode-covering vs mode-seeking behavior in ML</p>
            </header>

            <img src="./kl_image_resized.png" alt="KL Divergence visualization" class="blog-post-hero">

            <div class="blog-post-meta">
                <a href="https://github.com/hmhamad/KL-Divergence-BlogPost" target="_blank" class="github-link">
                    <i class="fab fa-github"></i> View Code on GitHub
                </a>
            </div>

            <div class="blog-post-tldr">
                <h3>TL;DR</h3>
                <p>Forward KL (mode-covering) and reverse KL (mode-seeking) produce fundamentally different behaviors when training models. Forward KL encourages the model to cover all modes of the target distribution, while reverse KL focuses on matching the highest probability regions. This choice has major implications for RLHF, distillation, and fine-tuning.</p>
            </div>

            <div class="blog-post-content">
                <p>If you've ever trained a variational autoencoder, fine-tuned a language model, or implemented policy optimization in reinforcement learning, you've encountered KL divergence. But here's the catch: there are two ways to compute it, and they lead to fundamentally different behaviors.</p>

                <p>Most machine learning practitioners know that KL divergence measures how different two probability distributions are. What's less obvious is that <strong>$D_{KL}(P||Q) \neq D_{KL}(Q||P)$</strong>. This asymmetry determines whether your model will be conservative or risky, whether it covers all modes or focuses on one, and ultimately shapes how your training behaves.</p>

                <p>While this topic has been discussed before, most explanations stay abstract. This post takes a different approach: we'll implement a toy example from scratch, visualize the actual optimization dynamics, and then systematically connect the behavior to diverse ML applications from language model pretraining to RLHF, VAEs, and knowledge distillation. By the end, you'll have both the intuition and the practical context to recognize which KL divergence you're using and why it matters.</p>

                <h2 id="what-is-kl">A Quick Reminder: What is KL Divergence?</h2>

                <p>In simple terms, KL divergence measures how one probability distribution differs from another:</p>

                $$D_{KL}(P||Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}$$

                <p>Or in the continuous case:</p>

                $$D_{KL}(P||Q) = \int P(x) \log \frac{P(x)}{Q(x)} dx$$

                <p style="position: relative;">It's always non-negative, and equals zero only when P and Q are identical. (Can you see why? Hint: Jensen's Inequality.<span class="sidenote-ref"></span>)<span class="sidenote"><span class="sidenote-number"></span> Jensen's inequality states that for a convex function f: E[f(X)] ≥ f(E[X]). Apply this to -log (which is convex) and the ratio P(x)/Q(x).</span></p>

                <p>Ok so KL feels like some kind of distance measure... or is it? How can it be a distance measure when it's not symmetric!</p>

                <p>In reality, KL measures how inefficient it is to pretend samples come from $Q$ when they really came from $P$. Think about it as some sort of a penalty. If you know the true distribution $P$, you get zero KL. Negative KL would mean you did better than the true distribution P, but since you can't do better on average → no negative KL.</p>

                <p>So to unwrap this asymmetry, we will do a simple experiment. But before that, let's give more context to the problem. Talking about probability distributions P and Q in abstract terms might be confusing, so let's connect it to something we already know.</p>

                <p>In the context of LLM training, think of it this way:</p>
                <ul>
                    <li><strong>P</strong>: the target distribution: could be your data distribution (your corpus), the reference model, or a teacher model</li>
                    <li><strong>Q</strong>: your model: the one you're actively training and updating, usually an LLM, policy/student/etc.</li>
                </ul>

                <p>For example:</p>
                <ul>
                    <li>In <strong>pretraining</strong> language models, P is the true data distribution and Q is your LLM learning to approximate it</li>
                    <li>In <strong>RLHF</strong>, P is your pre-trained base model (to prevent drift) and Q is your policy being optimized with RL (both LLMs in this case)</li>
                    <li>In <strong>knowledge distillation</strong>, P is the teacher model and Q is the student</li>
                </ul>

                <p>With this convention:</p>
                <ul>
                    <li><strong>Forward KL</strong>: $D_{KL}(P||Q) = \mathbb{E}_{x \sim P}\left[\log \frac{P(x)}{Q(x)}\right]$ becomes expectation under the target P</li>
                    <li><strong>Reverse KL</strong>: $D_{KL}(Q||P) = \mathbb{E}_{x \sim Q}\left[\log \frac{Q(x)}{P(x)}\right]$ becomes expectation under your model Q</li>
                </ul>

                <h2 id="experiment">The Experiment: Fitting a Unimodal to a Bimodal Distribution</h2>

                <p>Ok so what's the big deal? What happens when you choose one over the other? What do we usually use in practice?</p>

                <p>To answer all these questions, we'll do a simple toy experiment. Consider the following scenario:</p>
                <ul>
                    <li><strong>Target P</strong>: A bimodal Gaussian mixture with two equally weighted Gaussians centered at x = -2 and x = +6, each with unit variance</li>
                    <li><strong>Model Q</strong>: A single Gaussian with parameters $\mu$ (mean) and $\sigma$ (standard deviation) that we'll optimize</li>
                </ul>

                <p>This setup mirrors real ML problems: your data (or reference/teacher model) has multiple modes, but your model has limited capacity. How does the choice of KL divergence affect what Q learns?</p>

                <h3 id="setup">Setting Up the Distributions</h3>

                <p>First, let's define an abstract class for a probability distribution. We want it to have two methods:</p>
                <ul>
                    <li><code>sample</code>: Since we have expectations, we will need to do Monte Carlo estimation<span class="sidenote-ref"></span>, i.e., to generate and average over random samples from the distribution<span class="sidenote"><span class="sidenote-number"></span> Exact KL is rarely computed in modern ML systems. Expectations are intractable, so KL is usually estimated from samples, so we'll do that as well.</span></li>
                    <li><code>log_prob</code>: This method computes the log-probability of a given input under the distribution, simply to compute the $\log P(x)$ and $\log Q(x)$ that appear in the equations above.</li>
                </ul>

<pre><code class="language-python">class Distribution(ABC):
    """
    Abstract base class for probability distributions.
    All distributions must implement sample() and log_prob().
    """
    
    @abstractmethod
    def sample(self, n_samples):
        """Draw n_samples from the distribution."""
        pass
    
    @abstractmethod
    def log_prob(self, x):
        """Compute log probability density at x."""
        pass</code></pre>

                <p>Next, if you forgot your probabilities (shame on you), let's remind ourselves what a simple 1-D Gaussian Distribution looks like</p>

                $$x \sim \mathcal{N}(\mu, \sigma^2), \quad f(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\!\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$

                <p>where $\mu$ is the mean and $\sigma$ is the standard deviation. Now let's implement the two methods:</p>

                <p>
                <strong><code>sample</code></strong>:
                We could naively sample using a built-in function such as
                <code>torch.normal(mean=self.mu, std=self.sigma, size=(1, n_samples))</code>.
                This produces correct samples but it causes a problem once we start
                <em>training</em>.
                </p>

                <p>
                Remember that when optimizing our model <code>Q(x)</code>, we are actually
                learning the parameters <code>&mu;</code> and <code>&sigma;</code>.
                With direct sampling, the randomness is injected <em>inside</em> the sampling
                operation itself, which makes the output non-differentiable with respect to
                these parameters. As a result, gradients cannot properly flow back to
                <code>&mu;</code> and <code>&sigma;</code>.
                </p>

                <p style="position: relative;">
                To restore differentiability, we use the
                <strong>reparameterization trick</strong>:<span class="sidenote-ref"></span><span class="sidenote"><span class="sidenote-number"></span> This trick was popularized by Kingma & Welling in their 2014 VAE paper, enabling backpropagation through stochastic nodes.</span>
                </p>

                $$
                x = \mu + \sigma \,\epsilon,
                \qquad
                \epsilon \sim \mathcal{N}(0, 1)
                $$

                <p>
                Now the source of randomness is entirely isolated in
                <code>&epsilon;</code>, which is independent of the learnable parameters.
                The sampling step becomes a deterministic function of
                <code>&mu;</code> and <code>&sigma;</code>, allowing gradients to flow cleanly
                through the computation graph.
                </p>

                <p><strong><code>log_prob</code></strong>: Starting from the Gaussian density:</p>

                $$
                \log f(x) =
                \log \left(\frac{1}{\sigma\sqrt{2\pi}}
                \exp\!\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\right) =
                -\frac{1}{2}\log(2\pi)
                - \log \sigma
                - \frac{(x-\mu)^2}{2\sigma^2}
                $$

<pre><code class="language-python">class GaussianDistribution(Distribution):
    """Gaussian Distribution for gradient-based optimization."""
    
    def __init__(self, mu, sigma):
        self.mu = torch.as_tensor(mu, dtype=torch.float32).requires_grad_(True)
        self.sigma = torch.as_tensor(sigma, dtype=torch.float32).requires_grad_(True)
    
    def sample(self, n_samples):
        """Reparameterization trick: x = mu + sigma * eps"""
        eps = torch.randn(n_samples)
        return self.mu + self.sigma * eps
    
    def log_prob(self, x):
        """Compute log P(x) for torch tensors."""
        return -0.5 * torch.log(2 * torch.pi * self.sigma**2) - 0.5 * (x - self.mu)**2 / self.sigma**2</code></pre>

                <p>Now let's define our target distribution P(x), a <strong>mixture of Gaussians</strong>. A Gaussian Mixture Model (GMM) with K components can be written as</p>

                $$P(x) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(x \mid \mu_k, \sigma_k^2), \qquad \text{with } \sum_{k=1}^{K} \pi_k = 1$$

                where:
                <ul>
                    <li>$\pi_k$ are the mixture weights</li>    
                    <li>$\mu_k$ are the component means</li>    
                    <li>$\sigma_k$ are the component standard deviations</li>    
                </ul>

                <p><strong><code>sample</code></strong>: Drawing from a mixture follows the definition directly:</p>
                <ol>
                    <li>Sample a discrete component index $z \sim \mathrm{Categorical}(\pi_1,\ldots,\pi_K)$</li>
                    <li>Sample from the chosen Gaussian component $x \sim \mathcal{N}(\mu_z, \sigma_z^2)$</li>
                </ol>
                <p>In this experiment, $P(x)$ is a <strong>fixed reference distribution</strong>: we are not optimizing its parameters, so we don't need the sampling operation to be differentiable. That means we can use simple numpy sampling (fast and straightforward) without worrying about the reparameterization trick.</p>

                <p><strong><code>log_prob</code></strong>: Computing $\log P(x)$ requires care. Recall that for a mixture:</p>

                $$\log \left( \sum_{k} \pi_k p_k(x) \right) \neq \sum_{k} \pi_k \log p_k(x)$$

                <p>so we need to find a numerically stable log-sum-exp formulation. Let's derive it below:</p>

                <div class="math-block">
                        $$
                        \begin{aligned}
                        \log P(x)
                        &= \log\left(\sum_{k=1}^{K} \pi_k \, \mathcal{N}(x \mid \mu_k, \sigma_k^2)\right) \\[6pt]
                        &= \log\left(\sum_{k=1}^{K} \exp\Big(\log\left(\pi_k \, \mathcal{N}(x \mid \mu_k, \sigma_k^2)\right)\Big)\right) \\[6pt]
                        &= \log\left(\sum_{k=1}^{K} \exp\left(\log \pi_k + \log \mathcal{N}(x \mid \mu_k, \sigma_k^2)\right)\right) \\[6pt]
                        &= \log\left(\sum_{k=1}^{K} \exp\left(a_k(x)\right)\right),
                        \quad
                        \text{where } a_k(x) := \log \pi_k + \log \mathcal{N}(x \mid \mu_k, \sigma_k^2)
                        \end{aligned}
                        $$
                    </div>

                    <p style="position: relative;">
                        This expression is exactly a <strong>log-sum-exp</strong>.
                        Computing it naively can be numerically unstable when some
                        $a_k(x)$ values are very large or very small.<span class="sidenote-ref"></span><span class="sidenote"><span class="sidenote-number"></span> Without the max subtraction, exp(a_k) can overflow to infinity or underflow to zero, causing NaN gradients. This trick is standard in softmax implementations too.</span>
                    </p>

                    <p>
                        To stabilize the computation, we use the standard identity:
                    </p>

                    <div class="math-block">
                        $$
                        \begin{aligned}
                        \log \sum_{k=1}^{K} \exp(a_k)
                        &=
                        \log \sum_{k=1}^{K} \exp(m) \exp(a_k - m) \\[6pt]
                        &=
                        m + \log \sum_{k=1}^{K} \exp(a_k - m), \\[6pt]
                        \end{aligned}
                        $$
                    </div>

                    <p>
                        where $m = \max_k a_k$. This is what the code below implements.
                    </p>

<pre><code class="language-python">class GaussianMixtureDistribution(Distribution):
    def __init__(self, means, sigmas, weights):
        self.means = torch.tensor(means, dtype=torch.float32)
        self.sigmas = torch.tensor(sigmas, dtype=torch.float32)
        self.weights = torch.tensor(weights, dtype=torch.float32)
        self.weights = self.weights / self.weights.sum()  # normalize
        self.n_components = len(means)
    
    def sample(self, n_samples):
        """Sample from the mixture"""
        components = np.random.choice(
            self.n_components, 
            size=n_samples, 
            p=self.weights.numpy()
        )
        samples = np.random.normal(
            self.means.numpy()[components],
            self.sigmas.numpy()[components]
        )
        return torch.tensor(samples, dtype=torch.float32)
    
    def log_prob(self, x):
        """Compute log P(x) using log-sum-exp trick"""
        log_probs = []
        for i in range(self.n_components):
            log_gaussian = -0.5 * torch.log(2 * torch.pi * self.sigmas[i]**2) \
                          - 0.5 * (x - self.means[i])**2 / self.sigmas[i]**2
            log_probs.append(torch.log(self.weights[i]) + log_gaussian)
        
        log_probs = torch.stack(log_probs)
        max_log = torch.max(log_probs, dim=0)[0]
        return max_log + torch.log(torch.sum(torch.exp(log_probs - max_log), dim=0))</code></pre>

                <h3 id="computing-kl">Computing KL</h3>

                <p>Next, let's see how to compute the KL divergence in practice. Forward KL is defined as:</p>

                $$D_{KL}(P||Q) = \mathbb{E}_{x \sim P}[\log P(x) - \log Q(x)]$$

                <p>
                    The key detail is the distribution inside the expectation:
                    <strong>samples are drawn from $P$</strong>.
                </p>

                <p>
                    In practice, we approximate this expectation using Monte Carlo:
                </p>

                <ol>
                    <li>Sample $x^{(i)} \sim P(x)$</li>
                    <li>Evaluate $\log P(x^{(i)})$ and $\log Q(x^{(i)})$</li>
                    <li>Average the difference</li>
                </ol>

                <p>
                    We already have our handy functions, which makes the code very simple:
                </p>


<pre><code class="language-python">def forward_kl(p, q, n_samples=10000):
    """
    Compute KL(P||Q) = E_P[log P(x) - log Q(x)]
    Sample from P, evaluate both P and Q.
    """
    x = p.sample(n_samples)
    return torch.mean(p.log_prob(x) - q.log_prob(x))</code></pre>

                <p>Reverse KL swaps the roles:</p>

                $$D_{KL}(Q||P) = \mathbb{E}_{x \sim Q}[\log Q(x) - \log P(x)]$$

                <p>Now the expectation is taken under Q, i.e. we sample from the model itself.</p>

<pre><code class="language-python">def reverse_kl(p, q, n_samples=10000):
    """
    Compute KL(Q||P) = E_Q[log Q(x) - log P(x)]
    Sample from Q, evaluate both Q and P.
    """
    x = q.sample(n_samples)
    return torch.mean(q.log_prob(x) - p.log_prob(x))</code></pre>

                <h3 id="forward-kl">Forward KL: Minimizing KL(P||Q)</h3>

                <p>Finally! Let's get to the optimization part we've been waiting for. To keep things simple, we will not use a built-in PyTorch optimizer. Instead, we will implement simple gradient descent.

                    But we don't have to worry about computing the gradient ourselves. We can leave that part to PyTorch's autograd.</p>

                    <p>
                    <strong>By the way, if you're thinking to yourself:</strong>
                    “Wait… I've fine-tuned models before and I never explicitly used a KL loss?”
                    You're not wrong.
                    Since $\log P(x)$ is constant, doesn't depend on $Q$'s parameters, we can safely drop it and minimizing
                    $-\mathbb{E}_{x \sim P}[\log Q(x)]$
                    is equivalent, and that's just the familiar
                    <strong>cross-entropy (negative log-likelihood) loss</strong><span class="sidenote-ref"></span>
                    you're probably used to in PyTorch!<span class="sidenote"><span class="sidenote-number"></span> This is why you'll rarely see "forward KL" mentioned explicitly in pretraining code.</span>
                    </p>

<pre><code class="language-python">def optimize_forward_kl(p, mu_init=0.0, sigma_init=1.0, lr=0.1, n_steps=1000, n_samples=1000):
    """
    Optimize Q to minimize KL(P||Q) using gradient descent.
    
    Loss: E_P[-log Q(x)]
    Sample from P (fixed), compute -log Q(x), backprop through Q's parameters.
    """
    # Initialize Q's parameters
    mu = torch.tensor(mu_init, requires_grad=True)
    sigma = torch.tensor(sigma_init, requires_grad=True)
    
    loss_history = []
    
    for step in range(n_steps):
        # Create Q with current parameters
        q = GaussianDistribution(mu, sigma)
        
        # Compute loss: E_P[-log Q(x)]
        loss = forward_kl(p, q, n_samples)
        
        # Backprop
        loss.backward()
        
        # Gradient descent update
        with torch.no_grad():
            mu -= lr * mu.grad
            sigma -= lr * sigma.grad
            
            # Zero gradients
            mu.grad.zero_()
            sigma.grad.zero_()
        
        loss_history.append(loss.item())
        
        if step % 100 == 0:
            print(f"Step {step}: Loss = {loss.item():.4f}, mu = {mu.item():.4f}, sigma = {sigma.item():.4f}")
    
    return mu.item(), sigma.item(), loss_history</code></pre>

                <p><strong>Output:</strong></p>
<pre><code>Step 0: Loss = 8.8810, mu = 0.1893, sigma = 2.9083
Step 100: Loss = 0.7901, mu = 1.1521, sigma = 4.0272
Step 200: Loss = 0.7516, mu = 1.5349, sigma = 4.1245
Step 300: Loss = 0.7012, mu = 1.7554, sigma = 4.1338
Step 400: Loss = 0.7414, mu = 1.8741, sigma = 4.1251
Step 500: Loss = 0.7162, mu = 1.9375, sigma = 4.1203
Step 600: Loss = 0.7091, mu = 1.9555, sigma = 4.1231
Step 700: Loss = 0.7588, mu = 1.9747, sigma = 4.1216
Step 800: Loss = 0.7579, mu = 1.9890, sigma = 4.1182
Step 900: Loss = 0.7465, mu = 1.9980, sigma = 4.1202
Final parameters: mu = 1.9926, sigma = 4.1189</code></pre>

<p>Let's visualize!</p>

                <img src="./forward_kl_optimization.png" alt="Forward KL optimization showing mode-covering behavior" class="large">

                <p><strong>What happened?</strong> Recall our target P has modes at x = -2 and x = +6. Q learned to center itself at μ ≈ 2 (between the two modes) with a large σ ≈ 4.1 that <strong>covers both peaks</strong>.</p>

                <p><strong>Why?</strong> We're sampling from P and maximizing $\log Q(x)$, meaning we are asking $Q$ to explain the samples from $P$. So when x comes from either mode of $P$, $Q$ must assign reasonable probability to both regions. If $Q$ focused on just one mode, it would assign near-zero probability to samples from the other, leading to $\log Q(x) \rightarrow -\infty$.</p>

                <p>This is <strong>mode-covering behavior</strong>: forward KL penalizes Q heavily when Q(x) ≈ 0 but P(x) > 0.</p>

                <h3 id="reverse-kl">Reverse KL: Minimizing KL(Q||P)</h3>

                <p>Now let's check out what happens in the other direction.</p>
<pre><code class="language-python">def optimize_reverse_kl(p, mu_init=0.0, sigma_init=1.0, lr=0.01, n_steps=1000, n_samples=1000):
    """
    Optimize Q to minimize KL(Q||P) using gradient descent.
    
    Loss: E_Q[log Q(x) - log P(x)]
    Sample from Q using reparameterization, compute log Q(x) - log P(x), backprop.
    """
    # Initialize Q's parameters
    mu = torch.tensor(mu_init, requires_grad=True)
    sigma = torch.tensor(sigma_init, requires_grad=True)
    
    loss_history = []
    
    for step in range(n_steps):
        # Create Q with current parameters
        q = GaussianDistribution(mu, sigma)
        
        # Compute loss: E_Q[log Q(x) - log P(x)]
        loss = reverse_kl(p, q, n_samples)
        
        # Backprop
        loss.backward()
        
        # Gradient descent update
        with torch.no_grad():
            mu -= lr * mu.grad
            sigma -= lr * sigma.grad
            
            # Zero gradients
            mu.grad.zero_()
            sigma.grad.zero_()
        
        loss_history.append(loss.item())
        
        if step % 100 == 0:
            print(f"Step {step}: Loss = {loss.item():.4f}, mu = {mu.item():.4f}, sigma = {sigma.item():.4f}")
    
    return mu.item(), sigma.item(), loss_history</code></pre>

                <p><strong>Output:</strong></p>
<pre><code>Step 0: Loss = 2.6755, mu = -0.0181, sigma = 1.0046
Step 100: Loss = 0.9817, mu = -1.2403, sigma = 1.0532
Step 200: Loss = 0.7341, mu = -1.7184, sigma = 1.0109
Step 300: Loss = 0.6986, mu = -1.9010, sigma = 1.0004
Step 400: Loss = 0.6933, mu = -1.9620, sigma = 0.9978
Step 500: Loss = 0.6935, mu = -1.9849, sigma = 0.9978
Step 600: Loss = 0.6932, mu = -1.9952, sigma = 0.9995
Step 700: Loss = 0.6930, mu = -2.0038, sigma = 1.0004
Step 800: Loss = 0.6928, mu = -2.0021, sigma = 0.9946
Step 900: Loss = 0.6931, mu = -2.0001, sigma = 1.0030
Final parameters: mu = -2.0000, sigma = 1.0037</code></pre>

                <img src="./reverse_kl_optimization.png" alt="Reverse KL optimization showing mode-seeking behavior" class="large">

                <p><strong>Completely different!</strong> Q collapsed onto the left mode: $\mu ≈ -2$, $\sigma ≈ 1$. It ignored the right mode entirely.<span class="sidenote-ref"></span><span class="sidenote"><span class="sidenote-number"></span>We initialized μ = 0 (closer to the left mode) to break symmetry. Starting at $\mu = 2$ could converge to either mode. This initialization sensitivity is a real issue in practice. Reverse KL can get stuck in local optima, which is why techniques like annealing or multiple restarts are sometimes used.</span></p>

                <p>
                    <strong>Why?</strong>
                    We are sampling from $Q$ and evaluating $\log P(x)$.
                    In other words, we use $P$ to <em>score</em> samples proposed by $Q$.
                    Any mass that $Q$ places in low-probability regions of $P$
                    (e.g. between the modes, where $P(x) \approx 0$)
                    gets heavily penalized since $\log P(x) \to -\infty$.
                </p>

                <p>
                    The safest strategy is therefore to concentrate all probability mass
                    in a single high-density region of $P$.
                    To assign non-negligible mass to both modes, a unimodal Gaussian must widen, which inevitably increases mass in low-density regions (including between the modes). Reverse KL penalizes that.
                    The result: pick one mode and commit. Nothing to lose.
                </p>

                <p>This is <strong>mode-seeking behavior</strong>: reverse KL penalizes Q when Q(x) > 0 but P(x) ≈ 0.</p>
                

                <h2 id="takeaway">The Takeaway</h2>

                <p>Here's the intuition to remember:</p>

                <ul>
                    <li><strong>Forward KL</strong> $D_{KL}(P||Q)$: "Encourages <strong>behavioral cloning</strong>, prioritizes <strong>coverage</strong>" You sample from the target P and ask Q to explain those samples. If P has probability somewhere and Q doesn't, you get infinite penalty. Result: Q spreads out to cover all of P's modes, even if it means putting mass in low-probability regions <em>Mode-covering, conservative, recall-oriented.</em></li>
                    <li><strong>Reverse KL</strong> $D_{KL}(Q||P)$: "Encourages <strong>specialization</strong>, prioritizes <strong>precision</strong>" You sample from your model Q and ask P to validate those samples. If Q puts mass where P doesn't, you get infinite penalty. Result: Q collapses onto P's highest-density regions and ignores the rest. <em>Mode-seeking, confident, precision-oriented.</em></li>
                </ul>

                <div class="quiz-section">
                    <h2 id="quiz">Quiz Time!</h2>
                    <p>Let's apply what we learned to real ML problems.</p>

                    <!-- Quiz 1 -->
                    <div class="quiz-card" id="quiz1">
                        <div class="quiz-card-header">
                            <span class="quiz-number">Quiz 1</span>
                            <h3>Pre-training / SFT</h3>
                        </div>
                        <p class="quiz-question">You're training a gigantic LLM $Q_\theta(x)$ on the internet. The data distribution, call it $P_{data}$, is messy, multimodal, long-tailed, and full of weird edge cases. Which KL direction do you choose?</p>
                        <div class="quiz-options">
                            <div class="quiz-option correct" data-option="A">
                                <span class="quiz-option-letter">A</span>
                                <span class="quiz-option-text">Forward KL: $D_{KL}(P_{data}||Q_\theta)$ "Cover everything that actually shows up."</span>
                            </div>
                            <div class="quiz-option" data-option="B">
                                <span class="quiz-option-letter">B</span>
                                <span class="quiz-option-text">Reverse KL: $D_{KL}(Q_\theta||P_{data})$"Focus on the most likely stuff, ignore the rest."</span>
                            </div>
                        </div>
                        <button class="quiz-reveal-btn" onclick="revealAnswer('quiz1')">
                            <i class="fas fa-lightbulb"></i> Reveal Answer
                        </button>
                        <div class="quiz-answer">
                            <div class="quiz-answer-header">
                                <span class="quiz-answer-badge"><i class="fas fa-check"></i> Answer: A</span>
                            </div>
                            <div class="quiz-explanation">
                                <p><strong>Think about it:</strong> You have a massive corpus of Wikipedia, books, code, Reddit. Your goal is to learn a model that captures all of this. What matters most?</p>
                                <p>You cannot afford to miss real patterns. But if your model occasionally hallucinates? That's fine.</p>
                                <p>This screams forward KL: sample from the data, force the model to explain it. Miss something real → infinite penalty. Hallucinate something extra → no big deal.</p>
                                <p><strong>The math:</strong> The standard training objective is negative log-likelihood:</p>
                                $$\mathcal{L} = \mathbb{E}_{x \sim P_{data}}[-\log Q_\theta(x)]$$
                                <p>Which equals:</p>
                                $$D_{KL}(P_{data} || Q_\theta) = \text{constant} + \mathbb{E}_{x \sim P_{data}}[-\log Q_\theta(x)]$$
                                <p><strong>Bonus:</strong> Reverse KL wouldn't even work here! It requires sampling from $Q_\theta$ (random noise early on) and even worse, evaluating $P_{data}(x)$. But we don't have a density for the data, just samples! Forward KL is the only practical choice.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Quiz 2 -->
                    <div class="quiz-card" id="quiz2">
                        <div class="quiz-card-header">
                            <span class="quiz-number">Quiz 2</span>
                            <h3>RLHF Objective</h3>
                        </div>
                        <p class="quiz-question">Ok, you've pre-trained your model. Now you want to align it with human preferences. You have a reward model $R(x)$ trained on human feedback, and you're optimizing a policy $Q_\theta$. The core RL objective is</p>
                        $$\max_\theta\; \mathbb{E}_{x \sim Q_\theta}[R(x)]$$
                        <p class="quiz-question">Which KL direction does this resemble?</p>
                        <div class="quiz-options">
                            <div class="quiz-option" data-option="A">
                                <span class="quiz-option-letter">A</span>
                                <span class="quiz-option-text">Forward KL: Mode-covering behavior</span>
                            </div>
                            <div class="quiz-option correct" data-option="B">
                                <span class="quiz-option-letter">B</span>
                                <span class="quiz-option-text">Reverse KL: Mode-seeking behavior</span>
                            </div>
                        </div>
                        <button class="quiz-reveal-btn" onclick="revealAnswer('quiz2')">
                            <i class="fas fa-lightbulb"></i> Reveal Answer
                        </button>
                        <div class="quiz-answer">
                            <div class="quiz-answer-header">
                                <span class="quiz-answer-badge"><i class="fas fa-check"></i> Answer: B</span>
                            </div>
                            <div class="quiz-explanation">
                                <p><strong>Think about it:</strong> You want your model to produce high-reward outputs. You don't care about covering every possible good response. You just want to find the best ones and commit to them.</p>
                                <p>This is mode-seeking behavior. You're not trying to spread probability across all decent answers. You want to concentrate on the peaks of the reward landscape.</p>
                                <p>Notice the structure: you sample from your model $Q_\theta$, then evaluate those samples. That's exactly reverse KL's signature, expectation under Q.</p>
                                <p><strong>The math:</strong> If we define an "ideal" distribution $P^*(x) \propto \exp(R(x)/\beta)$ (high reward = high probability) then we can derive:</p>
                                $$D_{KL}(Q_\theta || P^*) = -\frac{1}{\beta}\mathbb{E}_{x \sim Q_\theta}[R(x)] + \text{constant}$$
                                <p>So maximizing expected reward is literally minimizing reverse KL to this reward-shaped target.</p>
                                <p class="quiz-note">
                                <strong>Important:</strong> This reverse KL is to an <em>implicit reward-shaped target distribution</em>, not to a reference model. In practice, we add a second KL term to keep the policy grounded, that's what comes next.
                                </p>
                            </div>
                        </div>
                    </div>

                    <!-- Quiz 3 -->
                    <div class="quiz-card" id="quiz3">
                        <div class="quiz-card-header">
                            <span class="quiz-number">Quiz 3</span>
                            <h3>RLHF Regularization</h3>
                        </div>
                        <p class="quiz-question">In practice, we add a KL penalty to keep the policy close to a reference model:</p>
                        $$\max_\theta\; \mathbb{E}_{x \sim Q_\theta}\big[R(x) - \beta \, D_{KL}(?)\big]$$
                        <p class="quiz-question">Which direction is commonly implemented for this KL penalty?</p>
                        <div class="quiz-options">
                            <div class="quiz-option" data-option="A">
                                <span class="quiz-option-letter">A</span>
                                <span class="quiz-option-text">Forward KL: $D_{KL}(Q_{\text{ref}} || Q_\theta)$</span>
                            </div>
                            <div class="quiz-option correct" data-option="B">
                                <span class="quiz-option-letter">B</span>
                                <span class="quiz-option-text">Reverse KL: $D_{KL}(Q_\theta || Q_{\text{ref}})$</span>
                            </div>
                        </div>
                        <button class="quiz-reveal-btn" onclick="revealAnswer('quiz3')">
                            <i class="fas fa-lightbulb"></i> Reveal Answer
                        </button>
                        <div class="quiz-answer">
                            <div class="quiz-answer-header">
                                <span class="quiz-answer-badge"><i class="fas fa-check"></i> Answer: B</span>
                            </div>
                            <div class="quiz-explanation">
                                <p><strong>Think about it:</strong> The KL penalty prevents your policy from going off the rails with reward hacking, generating gibberish that somehow scores high, etc. What behavior do we want?</p>
                                <p>We want to say: "You can focus on high-reward outputs (prune modes), but you cannot invent completely new behaviors the reference model wouldn't produce." Stay within the reference model's support.</p>
                                <p>That's reverse KL! $D_{KL}(Q_\theta || Q_{ref})$ penalizes Q when it puts mass where the reference doesn't. You can narrow down (mode-seek toward high reward), but you can't expand into territory the reference never explored.</p>
                                <p>Forward KL would do the opposite: force Q to cover everything the reference covers, defeating the purpose of RLHF (we want to specialize, not stay generic).</p>
                                <p class="quiz-note">
                                    <strong>Connection to the previous quiz:</strong> We're still using reverse KL but now the target is the <em>reference model</em>, not the reward distribution. Same KL direction, different constraint.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <h2 id="more-examples">More Examples</h2>
                <p>There are many more interesting examples: variational autoencoders, knowledge distillation, and out-of-distribution detection. Sometimes both forward and reverse KL may be viable depending on the data and desired outcome.</p>
                <p style="position: relative;">
                I want to share a nice real-world example in a paper I recently read where the direction of KL matters. The paper is trying to solve <em>machine unlearning</em>.<span class="sidenote-ref"></span>
                <span class="sidenote">
                <span class="sidenote-number"></span>
                Wang et al. “Balancing Forget Quality and Model Utility: A Reverse KL-Divergence Knowledge Distillation
                Approach for Better Unlearning in LLMs.” <em>NAACL 2025</em>.
                <a href="https://aclanthology.org/2025.naacl-long.60/" target="_blank">Paper</a>
                </span>
                Suppose a language model was trained on data containing a sensitive fact, such as
                "John Smith's SSN is 123-45-6789." The goal is to remove this specific fact while keeping the rest of the
                model intact and useful.
                </p>

                <p>
                The authors define a small <em>forget set</em> containing the data to be erased, and construct a
                <em>teacher distribution</em> that explicitly downweights tokens whose probabilities were boosted by this
                forget set (for example, the SSN digits), while leaving unrelated behavior unchanged. But the teacher model is not great, it has a deliberately distorted distribution meant only to guide learning.
                </p>

                <p>
                They then distill a student model from this teacher using <strong>reverse KL</strong>.
                Because reverse KL weights errors by the student's own probabilities, it strongly penalizes the model
                whenever it assigns high probability to forbidden tokens. If the student still tries to output
                “123-45-6789,” reverse KL “hammers” it. Forward KL, by contrast, would be more forgiving and could allow
                some leakage.
                </p>
                <h2 id="estimating-kl">Estimating KL in Practice</h2>

                <p style="position: relative;">A final note: When reading ML papers and code, you might be surprised to see KL written in a completely different form. For example, in the famous DeepSeek R1 paper,<span class="sidenote-ref"></span> they use this formula:<span class="sidenote"><span class="sidenote-number"></span> Guo et al. "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning." <em>arXiv 2025</em>. <a href="https://arxiv.org/abs/2501.12948" target="_blank">Paper</a></span></p>

                <img src="./deepseek_kl.png" alt="DeepSeek R1 KL formula">

                <p style="position: relative;">While this might seem weird, remember that in practice we always estimate KL using Monte Carlo sampling because computing the exact expectation is intractable.<span class="sidenote-ref"></span> The textbook discrete KL formula then becomes just one of many possible Monte Carlo estimators, and a naive one at that!</p>

                <p>This goes back to the age-old question of estimators: the <strong>bias-variance trade-off</strong>. The naive Monte Carlo estimator $\frac{1}{N}\sum_i \log \frac{P(x_i)}{Q(x_i)}$ can have high variance. A single sample where $Q(x)$ is very small (but $P(x)$ is not) can dominate the entire estimate and cause extreme variance.</p>

                <p style="position: relative;">For that reason, people look for better estimators. I won't expand more here as this has been extensively discussed. A good resource is John Schulman's blog post<span class="sidenote-ref"></span> where he tests different KL estimators. His conclusion? A particularly effective low-variance estimator adds a correction term to the naive estimator:<span class="sidenote"><span class="sidenote-number"></span> Schulman, J. "Approximating KL Divergence." <a href="http://joschu.net/blog/kl-approx.html" target="_blank">Blog post</a> a great deep dive into practical KL estimation.</span></p>

                $$\hat{D}_{KL}(P||Q) = (r - 1) - \log r \quad \text{where } r = \frac{P(x)}{Q(x)}$$

                <p>Look familiar? That's exactly what the DeepSeek paper uses :)</p>

                <!-- Citation Section -->
                <div class="citation-section">
                    <h2 id="citation">Citation</h2>
                    <p>Please cite this work as:</p>
                    <pre class="citation-box"><code>Hamad, Hassan, "Forward vs Reverse KL Divergence: Why the Direction Matters", hassanhamad.com, Jan 2026.</code><button class="citation-copy-btn" onclick="copyCitation(this)"><i class="fas fa-copy"></i></button></pre>
                    <p>Or use the BibTeX citation:</p>
                    <pre class="citation-box"><code>@article{hamad2026kl,
  author = {Hassan Hamad},
  title = {Forward vs Reverse KL Divergence: Why the Direction Matters},
  journal = {hassanhamad.com},
  year = {2026},
  note = {https://hassanhamad.com/blog/forward-vs-reverse-kl/},
}</code><button class="citation-copy-btn" onclick="copyCitation(this)"><i class="fas fa-copy"></i></button></pre>
                </div>

            </div>
        </div>
    </article>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2026 Hassan Hamad. All rights reserved.</p>
            <div class="footer-links">
                <a href="mailto:hmmhhamad@gmail.com"><i class="fas fa-envelope"></i></a>
                <a href="https://github.com/hmhamad" target="_blank"><i class="fab fa-github"></i></a>
                <a href="https://www.linkedin.com/in/hassan-hamad-338a7513b/" target="_blank"><i class="fab fa-linkedin"></i></a>
            </div>
        </div>
    </footer>

    <script src="../../js/scripts.js"></script>
</body>
</html>
